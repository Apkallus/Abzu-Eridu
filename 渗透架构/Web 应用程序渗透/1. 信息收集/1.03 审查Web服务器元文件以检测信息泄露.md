## 审查Web服务器元文件以检测信息泄露

### 摘要

通过测试各种元数据文件，以检测web应用程序路径或功能的信息泄露。此外，还可以将蜘蛛、机器人或爬虫避免的目录列表创建为[映射应用程序执行路径]()（信息收集 1.07节）的依赖项。还可以收集其他信息来识别攻击面、技术细节或用于社会工程活动。

### 目标

- 通过分析元数据文件，识别隐藏或混淆的路径与功能。
- 提取并映射其他信息，从而更深入地理解当前系统。

### 方法

```wget```执行的任何操作都可使用```curl```完成。许多动态应用安全测试（DAST）工具，如ZAP和Burp Suite，在其爬虫/蜘蛛功能中包含对这些资源的检查或解析。还可使用各种 Google Dorks 或利用高级搜索功能（如```inurl:```）进行识别。

#### 爬虫文件

Web spider、robot或crawler检索一个Web页面，然后递归地遍历超链接以检索更多的Web内容。它们被接受的行为由web根目录下的```robots.txt```文件中的 [Robots排除协议](https://www.robotstxt.org/) 指定。

例如，下面引用了 2025 年 12 月 22 日从 [Bing](https://cn.bing.com/robots.txt) 采集的 ```robots.txt``` 文件的开头部分：

```
User-agent: *
Disallow: /academic/profile
Disallow: /academic/search
Disallow: /account/
Disallow: /aclick
Disallow: /alink
Disallow: /amp/
Allow: /api/maps/
...
```

User-Agent指令指的是特定的网络蜘蛛/机器人/爬虫。例如， 
```User-Agent: Googlebot``` 指来自谷歌的蜘蛛，而 
```User-Agent: bingbot``` 指来自Microsoft的爬虫。 
```User-Agent: *``` 适用于所有网络蜘蛛/机器人/爬虫。

```Disallow``` 指令指定哪些资源对蜘蛛/机器人/爬虫禁止。在上面的例子中，以下内容是被禁止的：

```
...
Disallow: /academic/profile
Disallow: /academic/search
Disallow: /account/
Disallow: /aclick
Disallow: /alink
Disallow: /amp/
...
```

网络蜘蛛/机器人/爬虫可以故意忽略在 ```robots.txt``` 文件中指定的 ```Disallow``` 指令。因此， ```robots.txt``` 不应该被视为一种强制限制第三方如何访问、存储或重新发布web内容的机制。

```robots.txt``` 文件是从web服务器的web根目录中检索的。例如，要使用 ```wget``` 或 ```curl``` 从 ```cn.bing.com``` 中检索 ```robots.txt``` ：

```bash
$ curl -O -s https://cn.bing.com/robots.txt && head robots.txt
User-agent: msnbot-media
Disallow: /
Allow: /th?

User-agent: Twitterbot
Disallow:

User-agent: *
Disallow: /academic/profile
Disallow: /academic/search
```

#### 使用谷歌站长工具分析 robots.txt

网站所有者可以使用谷歌“Analyze robots.txt”功能来分析网站，作为其[谷歌站长工具](https://www.google.com/webmasters/tools)的一部分。该工具可以辅助测试，过程如下：

1. 用谷歌帐户登录谷歌网站管理员工具。
2. 在仪表板上，输入要分析的网站的URL。
3. 选择可用的方法，并遵循屏幕上的说明。

#### META 标签

```<META>``` 标签位于每个HTML文档的 ```HEAD``` 部分中，并且应该在整个站点中保持一致，以防机器人/蜘蛛/爬虫的起点是从网站根目录以外的文档链接（即深度链接）开始的情况。机器人指令也可以使用特定的[META标签](https://www.robotstxt.org/meta.html)来指定。

##### 机器人 META 标签